Directory tree, stemming from root "/home/caleb/repo/cosm-c360-tools":
├── README.md (169 lines)
├── TESTING.md (102)
├── cosmos.py (92)
├── debug.md (28)
├── dev.md (349)
├── pyproject.toml (11)
└── src (1319)
    │   └── cosmos (1319)
    │       │   ├── __init__.py (1)
    │       │   ├── cli.py (175)
    │       │   ├── manifest.py (231)
    │       │   ├── preflight.py (197)
    │       │   ├── processor.py (288)
    │       │   ├── utils.py (94)
    │       │   └── validation.py (333)
----
----
Full Path: cosmos.py

# File: cosmos.py

import sys
from pathlib import Path

from src.cosmos.cli import run_cli
from src.cosmos.preflight import run_self_test
from src.cosmos.utils import print_info, print_error, print_success
from src.cosmos.manifest import find_manifest, ManifestParser
from src.cosmos.validation import InputValidator
from src.cosmos.processor import VideoProcessor, ProcessingOptions, ProcessingMode

def main():
    config = run_cli()

    input_dir = Path(config["input_dir"])
    output_dir = Path(config["output_dir"])
    job_name = config.get("job_name", "default_job")

    # If self-test requested, run it and exit
    if config.get("self_test", False):
        manifest_path = Path(config["manifest"]) if "manifest" in config else None
        success = run_self_test(input_dir, output_dir, manifest_path)
        if success:
            print_success("Self-test passed. System ready.")
        else:
            print_error("Self-test failed. Please address the above issues.")
            sys.exit(1)
        sys.exit(0)

    # Attempt to find or load manifest
    manifest_path = config.get("manifest")
    if manifest_path:
        manifest_path = Path(manifest_path)
        # We'll trust the user on correctness. If invalid, validation will fail later.
        if not manifest_path.is_file():
            print_error(f"Specified manifest {manifest_path} not found.")
            sys.exit(1)
    else:
        # No manifest specified, try to find it automatically
        try:
            manifest_path = find_manifest(input_dir)
            if manifest_path is None:
                print_error("No manifest found. Please provide --manifest.")
                sys.exit(1)
        except ValueError as e:
            # Multiple manifests found
            print_error(str(e))
            print_error("Use --manifest to specify which manifest to use.")
            sys.exit(1)

    manifest_parser = ManifestParser(manifest_path)

    # Validate input directory and system readiness for actual processing
    validator = InputValidator(input_dir, output_dir, manifest_parser)
    validation_result = validator.validate_all()
    if not validation_result.can_proceed:
        print_error("Validation failed. Cannot proceed with processing.")
        for issue in validation_result.system_issues:
            print_error(f"System Issue: {issue.message}")
        sys.exit(1)

    # At this point, we have validated data and system. Proceed with processing.
    options = ProcessingOptions(
        output_resolution=(3840, 2160),
        quality_mode=ProcessingMode.BALANCED
    )
    processor = VideoProcessor(output_dir, options)

    # Process each valid clip
    for clip_name, clip_result in validation_result.clip_results.items():
        if clip_result.is_valid:
            result = processor.process_clip(clip_result)
            if result.success:
                print_success(f"Processed clip: {clip_name}. Output: {result.output_path}")
            else:
                print_error(f"Failed to process clip {clip_name}: {result.error}")

    # Write job info
    job_info_path = output_dir / "job_info.txt"
    with open(job_info_path, "w", encoding='utf-8') as f:
        f.write(f"Job Name: {job_name}\n")
        f.write(f"Input Directory: {input_dir}\n")
        f.write(f"Output Directory: {output_dir}\n")
        f.write(f"Manifest: {manifest_path}\n")
        f.write("Processing complete.\n")

    print_info("All done.")

if __name__ == "__main__":
    main()


----
Full Path: dev.md

# input directory structure/example metadata
a structured directory of .ts files (for data produced by a rather exotic ultra-high resolution camera, which saves four separate streams per file, and in approximately 1/10second chunks. 

You can get a good sense of the directory structure of the input data, and the two types of metadata available (a single clip manifest .xml, and meta.json files for each second) here:
```
Directory tree, stemming from root "/home/caleb/ladybird_failed_copy":
├── 0H (321 lines)
│   ├── 0M (180)
│   │   ├── 0S (3)
│   │   │   └── meta.json (3)
│   │   ├── 10S (3)
│   │   │   └── meta.json (3)
│   │   ├── 11S (3)
│   │   │   └── meta.json (3)
│   │   ├── 12S (3)
│   │   │   └── meta.json (3)
│   │   ├── 13S (3)
│   │   │   └── meta.json (3)
│   │   ├── 14S (3)
│   │   │   └── meta.json (3)
│   │   ├── 15S (3)
│   │   │   └── meta.json (3)
│   │   ├── 16S (3)
│   │   │   └── meta.json (3)
│   │   ├── 17S (3)
│   │   │   └── meta.json (3)
│   │   ├── 18S (3)
│   │   │   └── meta.json (3)
│   │   ├── 19S (3)
│   │   │   └── meta.json (3)
│   │   ├── 1S (3)
│   │   │   └── meta.json (3)
│   │   ├── 20S (3)
│   │   │   └── meta.json (3)
│   │   ├── 21S (3)
│   │   │   └── meta.json (3)
│   │   ├── 22S (3)
│   │   │   └── meta.json (3)
│   │   ├── 23S (3)
│   │   │   └── meta.json (3)
│   │   ├── 24S (3)
│   │   │   └── meta.json (3)
│   │   ├── 25S (3)
│   │   │   └── meta.json (3)
│   │   ├── 26S (3)
│   │   │   └── meta.json (3)
│   │   ├── 27S (3)
│   │   │   └── meta.json (3)
│   │   ├── 28S (3)
│   │   │   └── meta.json (3)
│   │   ├── 29S (3)
│   │   │   └── meta.json (3)
│   │   ├── 2S (3)
│   │   │   └── meta.json (3)
│   │   ├── 30S (3)
│   │   │   └── meta.json (3)
│   │   ├── 31S (3)
│   │   │   └── meta.json (3)
│   │   ├── 32S (3)
│   │   │   └── meta.json (3)
│   │   ├── 33S (3)
│   │   │   └── meta.json (3)
│   │   ├── 34S (3)
│   │   │   └── meta.json (3)
│   │   ├── 35S (3)
│   │   │   └── meta.json (3)
│   │   ├── 36S (3)
│   │   │   └── meta.json (3)
│   │   ├── 37S (3)
│   │   │   └── meta.json (3)
│   │   ├── 38S (3)
│   │   │   └── meta.json (3)
│   │   ├── 39S (3)
│   │   │   └── meta.json (3)
│   │   ├── 3S (3)
│   │   │   └── meta.json (3)
│   │   ├── 40S (3)
│   │   │   └── meta.json (3)
│   │   ├── 41S (3)
│   │   │   └── meta.json (3)
│   │   ├── 42S (3)
│   │   │   └── meta.json (3)
│   │   ├── 43S (3)
│   │   │   └── meta.json (3)
│   │   ├── 44S (3)
│   │   │   └── meta.json (3)
│   │   ├── 45S (3)
│   │   │   └── meta.json (3)
│   │   ├── 46S (3)
│   │   │   └── meta.json (3)
│   │   ├── 47S (3)
│   │   │   └── meta.json (3)
│   │   ├── 48S (3)
│   │   │   └── meta.json (3)
│   │   ├── 49S (3)
│   │   │   └── meta.json (3)
│   │   ├── 4S (3)
│   │   │   └── meta.json (3)
│   │   ├── 50S (3)
│   │   │   └── meta.json (3)
│   │   ├── 51S (3)
│   │   │   └── meta.json (3)
│   │   ├── 52S (3)
│   │   │   └── meta.json (3)
│   │   ├── 53S (3)
│   │   │   └── meta.json (3)
│   │   ├── 54S (3)
│   │   │   └── meta.json (3)
│   │   ├── 55S (3)
│   │   │   └── meta.json (3)
│   │   ├── 56S (3)
│   │   │   └── meta.json (3)
│   │   ├── 57S (3)
│   │   │   └── meta.json (3)
│   │   ├── 58S (3)
│   │   │   └── meta.json (3)
│   │   ├── 59S (3)
│   │   │   └── meta.json (3)
│   │   ├── 5S (3)
│   │   │   └── meta.json (3)
│   │   ├── 6S (3)
│   │   │   └── meta.json (3)
│   │   ├── 7S (3)
│   │   │   └── meta.json (3)
│   │   ├── 8S (3)
│   │   │   └── meta.json (3)
│   │   └── 9S (3)
│   │       │   └── meta.json (3)
│   └── 1M (141)
│       │   ├── 0S (3)
│       │   │   └── meta.json (3)
│       │   ├── 10S (3)
│       │   │   └── meta.json (3)
│       │   ├── 11S (3)
│       │   │   └── meta.json (3)
│       │   ├── 12S (3)
│       │   │   └── meta.json (3)
│       │   ├── 13S (3)
│       │   │   └── meta.json (3)
│       │   ├── 14S (3)
│       │   │   └── meta.json (3)
│       │   ├── 15S (3)
│       │   │   └── meta.json (3)
│       │   ├── 16S (3)
│       │   │   └── meta.json (3)
│       │   ├── 17S (3)
│       │   │   └── meta.json (3)
│       │   ├── 18S (3)
│       │   │   └── meta.json (3)
│       │   ├── 19S (3)
│       │   │   └── meta.json (3)
│       │   ├── 1S (3)
│       │   │   └── meta.json (3)
│       │   ├── 20S (3)
│       │   │   └── meta.json (3)
│       │   ├── 21S (3)
│       │   │   └── meta.json (3)
│       │   ├── 22S (3)
│       │   │   └── meta.json (3)
│       │   ├── 23S (3)
│       │   │   └── meta.json (3)
│       │   ├── 24S (3)
│       │   │   └── meta.json (3)
│       │   ├── 25S (3)
│       │   │   └── meta.json (3)
│       │   ├── 26S (3)
│       │   │   └── meta.json (3)
│       │   ├── 27S (3)
│       │   │   └── meta.json (3)
│       │   ├── 28S (3)
│       │   │   └── meta.json (3)
│       │   ├── 29S (3)
│       │   │   └── meta.json (3)
│       │   ├── 2S (3)
│       │   │   └── meta.json (3)
│       │   ├── 30S (3)
│       │   │   └── meta.json (3)
│       │   ├── 31S (3)
│       │   │   └── meta.json (3)
│       │   ├── 32S (3)
│       │   │   └── meta.json (3)
│       │   ├── 33S (3)
│       │   │   └── meta.json (3)
│       │   ├── 34S (3)
│       │   │   └── meta.json (3)
│       │   ├── 35S (3)
│       │   │   └── meta.json (3)
│       │   ├── 36S (3)
│       │   │   └── meta.json (3)
│       │   ├── 37S (3)
│       │   │   └── meta.json (3)
│       │   ├── 38S (3)
│       │   │   └── meta.json (3)
│       │   ├── 39S (3)
│       │   │   └── meta.json (3)
│       │   ├── 3S (3)
│       │   │   └── meta.json (3)
│       │   ├── 40S (3)
│       │   │   └── meta.json (3)
│       │   ├── 41S (3)
│       │   │   └── meta.json (3)
│       │   ├── 42S (3)
│       │   │   └── meta.json (3)
│       │   ├── 43S (3)
│       │   │   └── meta.json (3)
│       │   ├── 44S (3)
│       │   │   └── meta.json (3)
│       │   ├── 45S (3)
│       │   │   └── meta.json (3)
│       │   ├── 46S (3)
│       │   │   └── meta.json (3)
│       │   ├── 47S (3)
│       │   │   └── meta.json (3)
│       │   ├── 48S (3)
│       │   │   └── meta.json (3)
│       │   ├── 49S (3)
│       │   │   └── meta.json (3)
│       │   ├── 4S (3)
│       │   │   └── meta.json (3)
│       │   ├── 50S (3)
│       │   │   └── meta.json (3)
│       │   ├── 51S (3)
│       │   │   └── meta.json (3)
│       │   └── 52S (0)
└── LADYBIRD.xml (8)
----
----
Full Path: LADYBIRD.xml

<Clip_Manifest NumDirs="498">
  <_1 Name="CLIP2" In="1.6261465427355111E-307" InIdx="228" Out="1.6261466707242047E-307" OutIdx="4110" Locked="True" InStr="07:27:16.618 08/13/2024" Epoch="1723559236.618" Pos="0H/0M/3.8S/" />
  <_2 Name="CLIP1" In="1.6261465850368715E-307" InIdx="1511" Out="1.6261470057932999E-307" OutIdx="14273" Locked="True" InStr="07:27:38.022 08/13/2024" Epoch="1723559258.0219998" Pos="0H/0M/25.1833333333333S/" />
  <_3 Name="CLIP3" In="1.6261501214407288E-307" InIdx="14658" Out="1.6261501214407288E-307" OutIdx="14658" Locked="True" InStr="07:57:27.463 08/13/2024" Epoch="1723561047.4629998" Pos="0H/4M/4.30000000000001S/" />
  <_4 Name="CLIP4" In="1.6261504029336424E-307" InIdx="15096" Out="1.6261504567393675E-307" OutIdx="16728" Locked="True" InStr="07:59:49.900 08/13/2024" Epoch="1723561189.8999999" Pos="0H/4M/11.6S/" />
  <_5 Name="CLIP6" In="1.626151890120643E-307" InIdx="17124" Out="1.6261520105896575E-307" OutIdx="20778" Locked="True" InStr="08:12:22.425 08/13/2024" Epoch="1723561942.425" Pos="0H/4M/45.4S/" />
  <_6 Name="CLIP7" In="1.6261529151745092E-307" InIdx="21221" Out="1.6261531935026773E-307" OutIdx="29663" Locked="True" InStr="08:21:01.108 08/13/2024" Epoch="1723562461.108" Pos="0H/5M/53.6833333333333S/" />
</Clip_Manifest>

----
Full Path: 0H/1M/43S/meta.json

{
"Time":{"x0":1723559335.914, "xi-x0":[0,0.017,0.033,0.050,0.067,0.083,0.100,0.117,0.133,0.150,0.167,0.183,0.200,0.217,0.233,0.250,0.267,0.284,0.300,0.317,0.334,0.350,0.367,0.384,0.400,0.417,0.434,0.450,0.467,0.484,0.500,0.517,0.534,0.550,0.567,0.584,0.601,0.617,0.634,0.651,0.667,0.684,0.701,0.717,0.734,0.751,0.767,0.784,0.801,0.817,0.834,0.851,0.867,0.884,0.901,0.917,0.934,0.951,0.968,0.984]}
}

----
Full Path: 0H/1M/44S/meta.json

{
"Time":{"x0":1723559336.915, "xi-x0":[0,0.017,0.033,0.050,0.067,0.083,0.100,0.117,0.133,0.150,0.167,0.183,0.200,0.217,0.233,0.250,0.267,0.284,0.300,0.317,0.334,0.350,0.367,0.384,0.400,0.417,0.434,0.450,0.467,0.484,0.500,0.517,0.534,0.550,0.567,0.584,0.600,0.617,0.634,0.651,0.667,0.684,0.701,0.717,0.734,0.751,0.767,0.784,0.801,0.817,0.834,0.851,0.867,0.884,0.901,0.917,0.934,0.951,0.968,0.984]}
}
```


---

# COSM C360 Tools Project Context

## Overview
The COSM C360 Tools project (`cosmos`) is a utility for converting specialized video output from COSM C360 cameras (commonly used in professional sports production, e.g., NFL pylon cameras) into standard MP4 format. The tool processes the camera's unique output format, which consists of multiple tiled HEVC streams with accompanying metadata.

## Target Users
- **Primary Users**: Ecologists and scientists deploying these cameras for field research
- **Technical Level**: Non-technical users who need to review footage before submitting to main analysis pipeline
- **Use Case**: Converting raw camera format to standard MP4s for review and ingest into ecological analysis tools

## User Considerations
1. **Technical Expertise**
   - Users are not technically sophisticated
   - Need simple, guided interface
   - Should handle errors gracefully with clear explanations
   - Must provide clear feedback about processing status

2. **System Requirements**
   - Must support Windows (primary platform for users)
   - Must work on low-end systems (not workstation-grade machines)
   - Should provide options for memory-constrained systems
   - Should automatically detect and use best available encoding options

3. **Workflow Integration**
   - Tool will eventually be integrated into main ecological analysis software
   - Currently needed as standalone utility for pre-processing
   - Should maintain compatibility with analysis pipeline requirements

## Key Requirements Beyond Technical Specs

1. **Usability**
   - Interactive mode for guided operation
   - Clear progress indication
   - Informative error messages
   - Pre-flight validation to catch issues early
   - No technical knowledge required for basic operation

2. **Robustness**
   - Automatic encoder selection and fallback
   - Handles partial or corrupted data gracefully
   - Works across different platforms
   - Adapts to available system resources

3. **System Adaptation**
   - Automatic detection of optimal settings
   - Low-memory mode for constrained systems
   - Fallback to software encoding if hardware encoding unavailable
   - Clear warnings about system requirements

4. **Error Handling**
   - Clear, non-technical error messages
   - Validation before processing starts
   - Helpful suggestions for resolving issues
   - Graceful handling of partial or corrupted data

## Development Priorities
1. **Core Functionality**
   - Reliable video processing
   - Robust input validation
   - Cross-platform compatibility
   - Memory usage optimization

2. **User Experience**
   - Simple, guided interface
   - Clear progress reporting
   - Informative feedback
   - Error resilience

3. **Optional Enhancements**
   - Configuration saving
   - Processing resumption
   - Automated updates
   - Additional format support

## Deployment Considerations
- Tool will be distributed via public repository
- Users will need to pull repo and follow installation instructions
- Should minimize external dependencies
- Must include clear installation and usage documentation
- Should provide support for common issues

## Integration Context
This tool is part of a larger ecological analysis pipeline:
1. Raw footage captured with COSM C360 cameras
2. Conversion to standard MP4 format (this tool)
3. Ingestion into ecological analysis software
4. Analysis of ecological activity in footage

## Future Considerations
- Tool may be integrated directly into main analysis software
- May need to handle additional camera formats
- Could expand to support batch processing
- Might need additional output format options

----
Full Path: pyproject.toml

[project]
name = "cosm-c360-tools"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "psutil>=6.1.0",
    "questionary>=2.0.1",
]


----
Full Path: TESTING.md

# Testing the COSM C360 Tools

This document outlines how to run tests on macOS, Linux, and Windows.

## Requirements

- Python 3.10 or higher
- `pytest` installed in your environment
- `ffmpeg` installed and accessible in `PATH`
- The input directory specified for integration tests (`/datasets/dataZoo/clients/ladybird_data/LADYBIRD/failed_copy`) must be accessible on your machine.
- The output directory (`/datasets/dataZoo/clients/ladybird_data/LADYBIRD/cosmos_out`) must exist and be writable.

## Installing Test Dependencies

If you used `uv` (recommended):

```bash
# On macOS and Linux
curl -LsSf https://astral.sh/uv/install.sh | sh
uv new cosmos-env
uv activate cosmos-env
pip install -r requirements.txt

# On Windows (PowerShell)
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
uv new cosmos-env
uv activate cosmos-env
pip install -r requirements.txt
```

If not using `uv`, you can use `python -m venv` and `pip` as documented in the README.

## Running Tests

To run all tests:
```bash
pytest tests
```

This will run:
- Unit tests for manifest, validation, processing.
- The integration test (`test_integration.py`).

If the integration test fails, ensure that:
- The specified input and output directories are accessible.
- FFmpeg is installed and in `PATH`.
- You have sufficient system resources (disk space, etc.).

## Platform Notes

- **macOS & Linux**: Should work out of the box with `pytest`.
- **Windows**: Ensure Python and ffmpeg are installed, and that `ffmpeg` is in `PATH`. Running `pytest` in `powershell` or `cmd` is supported.

## Additional Tips

- Use `-v` for verbose output:
  ```bash
  pytest -v tests
  ```
- Use `-k` to run a specific test:
  ```bash
  pytest -k test_self_test_integration
  ```

## Inspecting Output Files

To inspect the processed MP4 files and verify their contents, you can use FFmpeg's ffprobe tool:

```bash
# Get detailed information about the video file
ffprobe -v error -show_format -show_streams output.mp4

# Get just the duration and basic format info
ffprobe -v error -show_entries format=duration,size,bit_rate -of default=noprint_wrappers=1 output.mp4

# Show frame information (useful for debugging black frames)
ffprobe -v error -show_frames -of compact output.mp4

# Quick visual check of frames (requires display)
ffplay output.mp4
```

Common issues to check for:
- Duration matches expected clip length
- Video codec is h264
- Frame rate is correct (typically 59.94/60fps for COSM footage)
- Resolution matches expected output (e.g., 3840x2160)
  - COSM output is 9280x6300 (total of the four streams is 9344x6364, but we crop 32px from interior of each stream)
- Presence of video stream (missing video stream but present audio can indicate encoding issues)

Example of checking a specific file:
```bash
# Check if file contains valid video stream
ffprobe -v error -select_streams v:0 -show_entries stream=codec_name,width,height,r_frame_rate -of default=noprint_wrappers=1 output.mp4
```

# DEV
Copy test results from blade to local machine:
```bash
rsync -avz caleb@blade:/datasets/dataZoo/clients/ladybird_data/LADYBIRD/cosmos_out/ ~/cosmos_tests/test0/
```


----
Full Path: README.md

# COSM C360 Tools

**COSM C360 Tools** (referred to as `cosmos`) is a command-line utility for converting specialized video output from COSM C360 cameras into standard MP4 video files suitable for review and downstream analysis. It handles the complex tiled HEVC output and merges multiple streams into a standard video format.

## Key Features

- **Manifest Parsing & Validation**: Automatically finds and parses the camera's XML manifest file, identifying clips and verifying temporal continuity.
- **Segment Analysis**: Gathers `.ts` segments and their corresponding `meta.json` files, ensuring integrity and completeness.
- **Video Assembly**: Extracts and aligns four HEVC tile streams, handles overlapping regions, and assembles frames into a full-resolution video.
- **Multi-Resolution Output**: Outputs a full-resolution master file and can generate downscaled variants (e.g., 4K, 1080p).
- **Cross-Platform & Hardware Acceleration**: Works on Linux, macOS, and Windows. Automatically attempts hardware-accelerated encoding (NVENC, AMF, QSV) and falls back to software (x264).
- **Pre-Flight Checks**: Built-in `--self-test` mode checks system requirements (FFmpeg, disk space, memory) and input directory structure before processing.
- **Interactive & Non-Interactive Modes**: Fully scriptable via CLI flags, or run interactively with guided prompts for non-technical users.
- **Update Checking**: Checks for updates if `git` is available.

## System Requirements

- **Operating System**: Linux, macOS, or Windows.
- **Python**: 3.10 or higher recommended.
- **FFmpeg**: Must be installed and accessible in `PATH`.
- **Memory**: At least 8GB RAM recommended (less may cause slowdowns, consider `--low-memory` mode).
- **Disk Space**: At least 10GB free disk space recommended.
- **Git (Optional)**: To automatically check for updates when running `--check-updates`.

## Input Directory Structure

The input directory should follow a hierarchical structure reflecting the camera output:

```
input_dir/
 ├── 0H/
 │   ├── 0M/
 │   │   ├── 0S/
 │   │   │   ├── meta.json
 │   │   │   ├── <segment .ts files>
 │   │   ├── 1S/
 │   │   │   ├── meta.json
 │   │   │   ├── <segment .ts files>
 │   │   └── ...
 │   └── 1M/
 │       ├── 0S/
 │       │   ├── meta.json
 │       │   ├── <segment .ts files>
 │       └── ...
 ├── MyManifest.xml
 └── AnotherManifest.xml (if multiple manifests exist)
```

- Each second-level directory (`XH/XM/XS`) must contain a `meta.json` file and `.ts` files.
- The top-level directory should contain a single `.xml` manifest file. If multiple `.xml` files are present, you must specify which one to use via `--manifest`.

## Installation

We recommend using [UV](https://github.com/astral-sh/uv?tab=readme-ov-file) for environment management.

### Using UV (Recommended)

**macOS / Linux**:
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
uv new cosmos-env
uv activate cosmos-env
pip install -r requirements.txt
```

**Windows (PowerShell)**:
```powershell
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
uv new cosmos-env
uv activate cosmos-env
pip install -r requirements.txt
```

If not using UV, you can still use Python's built-in `venv`:
```bash
# On macOS/Linux:
python3 -m venv venv
source venv/bin/activate

# On Windows:
python -m venv venv
venv\Scripts\activate

pip install -r requirements.txt
```

Confirm `ffmpeg` is installed and in your PATH:
```bash
ffmpeg -version
```

If `ffmpeg` is not found, please install it according to your platform's instructions.

## Usage

### Non-Interactive Mode

Run `cosmos.py` directly or via `python`:

```bash
python cosmos.py --input-dir /path/to/input --output-dir /path/to/output
```

If multiple `.xml` manifests exist, specify which one:
```bash
python cosmos.py --input-dir /path/to/input --output-dir /path/to/output --manifest /path/to/manifest.xml
```

Run a system self-test before processing:
```bash
python cosmos.py --input-dir /path/to/input --output-dir /path/to/output --self-test
```

Check for updates:
```bash
python cosmos.py --check-updates
```

Specify a job name (useful for logs and output notes):
```bash
python cosmos.py --input-dir /path/to/input --output-dir /path/to/output --job-name MyJobName
```

Adjust logging:
```bash
python cosmos.py --input-dir /path/to/input --output-dir /path/to/output --log-level DEBUG --log-file mylog.log
```

### Interactive Mode

If you're unsure about the input directory or other parameters, run:
```bash
python cosmos.py --interactive
```

The tool will guide you through selecting the input directory, output directory, and other settings interactively. After confirming, it will proceed with processing.

### Example Commands (All Platforms)

**Windows (PowerShell)**:
```powershell
python .\cosmos.py --input-dir C:\data\cosm_input --output-dir C:\data\cosm_output --self-test
```

**macOS / Linux (Bash)**:
```bash
python cosmos.py --input-dir ~/cosm_input --output-dir ~/cosm_output --interactive
```

## Running Tests

Tests are included to ensure code quality and correctness:

```bash
pytest tests
```

Ensure `pytest` is installed:
```bash
pip install pytest
```

This runs the test suite (unit tests for validation, processing, manifest parsing, and integration tests). Use `TESTING.md` for more detailed testing instructions.

## Additional Notes

- Output files, including `job_info.txt`, are placed in the specified `output` directory.
- If system resources are limited, consider `--low-memory` mode (to be added) or reducing resolution.
- If you wish to contribute or request features, please open an issue or a pull request.

----
Full Path: debug.md

The output clips have all-black frames. We need to narrow down the cause.
- The filter complex we are using here has no meaningful difference from the filter complex in the previous (known-good) version from the standalone version of the script.






# DEBUGGING

## Test Each Tile Individually:
Extract and encode just [0:v:0] without cropping:

```bash
ffmpeg -i /datasets/dataZoo/clients/ladybird_data/LADYBIRD/failed_copy/0H/0M/0S/0.ts -map 0:v:0 -c copy single_tile_0.mp4 \
&& ffmpeg -i /datasets/dataZoo/clients/ladybird_data/LADYBIRD/failed_copy/0H/0M/0S/0.ts -map 0:v:1 -c copy single_tile_1.mp4 \
&& ffmpeg -i /datasets/dataZoo/clients/ladybird_data/LADYBIRD/failed_copy/0H/0M/0S/0.ts -map 0:v:2 -c copy single_tile_2.mp4 \
&& ffmpeg -i /datasets/dataZoo/clients/ladybird_data/LADYBIRD/failed_copy/0H/0M/0S/0.ts -map 0:v:3 -c copy single_tile_3.mp4
```
Check if single_tile_0.mp4 shows any content. Repeat for [0:v:1], [0:v:2], and [0:v:3].

**RESULT**:
- single_tile_0.mp4 shows content
- single_tile_1.mp4 shows content
- single_tile_2.mp4 shows content
- single_tile_3.mp4 shows content
All look correct.


----
Full Path: src/cosmos/validation.py

from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple
import json
import shutil
import subprocess
from datetime import datetime
import logging

from .manifest import ClipInfo, ClipStatus, Position, ManifestParser

class ValidationLevel(Enum):
    """Severity level for validation issues"""
    ERROR = "error"          # Fatal issue, cannot proceed
    WARNING = "warning"      # Potential issue, can proceed with caution
    INFO = "info"           # Informational note

@dataclass
class ValidationIssue:
    """Details about a validation problem"""
    level: ValidationLevel
    message: str
    context: Optional[str] = None
    help_text: Optional[str] = None

@dataclass
class SegmentInfo:
    """Information about a video segment from meta.json"""
    directory: Path
    start_time: float
    frame_timestamps: List[float]
    ts_files: List[Path]
    
    @property
    def end_time(self) -> float:
        """Get end time of segment from last frame timestamp"""
        return self.frame_timestamps[-1] if self.frame_timestamps else self.start_time
    
    @property
    def frame_count(self) -> int:
        """Number of frames in segment"""
        return len(self.frame_timestamps)
    
    @property
    def has_all_files(self) -> bool:
        """Check if all expected .ts files are present"""
        return len(self.ts_files) == self.frame_count

@dataclass
class ClipValidationResult:
    """Validation results for a single clip"""
    clip: ClipInfo
    segments: List[SegmentInfo]
    missing_segments: List[Position]
    issues: List[ValidationIssue]
    estimated_size: int  # in bytes
    
    @property
    def is_valid(self) -> bool:
        """Check if clip has enough valid data to process"""
        return bool(self.segments) and not any(
            issue.level == ValidationLevel.ERROR 
            for issue in self.issues
        )

@dataclass
class ValidationResult:
    """Complete validation results for input directory"""
    system_issues: List[ValidationIssue]
    clip_results: Dict[str, ClipValidationResult]
    total_size_estimate: int
    available_space: int
    
    @property
    def can_proceed(self) -> bool:
        """Check if processing can proceed with any clips"""
        return (
            not any(i.level == ValidationLevel.ERROR for i in self.system_issues)
            and any(r.is_valid for r in self.clip_results.values())
            and self.available_space > self.total_size_estimate
        )

class InputValidator:
    """
    Validates input data and system requirements for COSM video processing.
    
    Performs checks at multiple levels:
    1. System requirements (ffmpeg, space, etc.)
    2. Input directory structure
    3. Clip data integrity
    4. Segment availability and validity
    """
    
    def __init__(self, 
                 input_dir: Path,
                 output_dir: Path,
                 manifest_parser: ManifestParser):
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.manifest_parser = manifest_parser
        self.logger = logging.getLogger(__name__)
        
    def validate_system(self) -> List[ValidationIssue]:
        """Check system requirements"""
        issues = []
        
        # Check ffmpeg installation
        try:
            subprocess.run(
                ["ffmpeg", "-version"], 
                capture_output=True, 
                check=True
            )
        except subprocess.CalledProcessError:
            issues.append(ValidationIssue(
                level=ValidationLevel.ERROR,
                message="FFmpeg not found",
                help_text="Please install FFmpeg to process videos"
            ))
        except FileNotFoundError:
            issues.append(ValidationIssue(
                level=ValidationLevel.ERROR,
                message="FFmpeg not found in system PATH",
                help_text="Please install FFmpeg and ensure it's in your PATH"
            ))
            
        # Check output directory
        try:
            self.output_dir.mkdir(parents=True, exist_ok=True)
            test_file = self.output_dir / ".write_test"
            test_file.touch()
            test_file.unlink()
        except Exception as e:
            issues.append(ValidationIssue(
                level=ValidationLevel.ERROR,
                message=f"Cannot write to output directory: {e}",
                help_text="Check permissions and disk space"
            ))
            
        return issues
    
    def validate_segment(self, 
                        segment_dir: Path) -> Optional[SegmentInfo]:
        """
        Validate a segment directory and its meta.json
        
        Args:
            segment_dir: Path to segment directory
            
        Returns:
            SegmentInfo if valid, None if invalid
        """
        meta_path = segment_dir / "meta.json"
        self.logger.debug(f"Validating segment at {segment_dir}")
        
        if not meta_path.is_file():
            self.logger.debug(f"No meta.json found in {segment_dir}")
            return None
        
        try:
            with open(meta_path) as f:
                meta = json.load(f)
                self.logger.debug(f"Loaded meta.json: {meta}")
                
            if "Time" not in meta or "x0" not in meta["Time"] or "xi-x0" not in meta["Time"]:
                self.logger.error(f"Invalid meta.json structure in {segment_dir}")
                return None
                
            start_time = meta["Time"]["x0"]
            increments = meta["Time"]["xi-x0"]
            
            # Get all .ts files in directory
            ts_files = sorted(segment_dir.glob("*.ts"))
            self.logger.debug(f"Found {len(ts_files)} .ts files in {segment_dir}")
            
            # Create timestamps for each frame
            timestamps = [start_time + inc for inc in increments]
            
            # Validate frame count matches file count
            if len(ts_files) != len(increments):
                self.logger.warning(
                    f"Mismatch in {segment_dir}: "
                    f"{len(ts_files)} .ts files but {len(increments)} frame timestamps"
                )
            
            self.logger.debug(
                f"Segment validation successful: "
                f"start_time={start_time}, "
                f"frame_count={len(timestamps)}, "
                f"file_count={len(ts_files)}"
            )
            
            return SegmentInfo(
                directory=segment_dir,
                start_time=start_time,
                frame_timestamps=timestamps,
                ts_files=ts_files
            )
            
        except (json.JSONDecodeError, KeyError, TypeError) as e:
            self.logger.warning(f"Warning: failed to parse meta.json in {segment_dir}: {e}")
            # Note: this might be better as a .debug log, but we should tell user proportion of failed segments for each clip in the manifest
            return None
    
    def validate_clip(self, clip: ClipInfo) -> ClipValidationResult:
        """Validate all segments for a clip"""
        issues = []
        segments = []
        missing_positions = []
        
        self.logger.debug(f"Validating clip: {clip.name}")
        self.logger.debug(
            f"Clip boundaries: "
            f"start={clip.start_pos.to_string()}, "
            f"frames={clip.start_idx}-{clip.end_idx}"
        )

        start_sec = int(clip.start_pos.second)
        end_sec = int(clip.end_pos.second) if clip.end_pos else start_sec + 60
        total_positions = end_sec - start_sec + 1
        
        self.logger.debug(f"Scanning {total_positions} second positions")

        last_success_pos = None
        for second in range(start_sec, end_sec + 1):
            pos = Position(
                hour=clip.start_pos.hour,
                minute=clip.start_pos.minute,
                second=second
            )
            
            segment_dir = self.input_dir / pos.path_fragment()
            self.logger.debug(f"Checking position {pos.to_string()} -> {segment_dir}")
            
            if not segment_dir.is_dir():
                self.logger.debug(f"Missing segment directory: {segment_dir}")
                missing_positions.append(pos)
                continue

            if segment_info := self.validate_segment(segment_dir):
                self.logger.debug(
                    f"Valid segment found: {len(segment_info.ts_files)} files, "
                    f"{len(segment_info.frame_timestamps)} frames"
                )
                segments.append(segment_info)
                last_success_pos = pos
            else:
                self.logger.debug(f"Invalid segment at {segment_dir}")
                issues.append(ValidationIssue(
                    level=ValidationLevel.WARNING,
                    message=f"Invalid segment at {pos.path_fragment()}",
                    context="Missing or corrupt meta.json"
                ))

        # Report segment coverage
        found_segments = len(segments)
        if found_segments == 0:
            self.logger.warning(f"No valid segments found for clip {clip.name}")
        else:
            coverage = (found_segments / total_positions) * 100
            self.logger.info(
                f"Clip {clip.name}: Found {found_segments}/{total_positions} "
                f"segments ({coverage:.1f}% coverage)"
            )
            for segment in segments:
                self.logger.debug(
                    f"Segment {segment.directory.name}: "
                    f"{len(segment.ts_files)} files, "
                    f"time range: {segment.start_time:.3f}-{segment.end_time:.3f}"
                )

        # Estimate output size (very rough)
        estimated_size = len(segments) * 100 * 1024 * 1024

        # Update clip end info if we have segments
        if segments:
            clip.end_epoch = segments[-1].end_time
            if last_success_pos:
                clip.end_pos = last_success_pos

        return ClipValidationResult(
            clip=clip,
            segments=segments,
            missing_segments=missing_positions,
            issues=issues,
            estimated_size=estimated_size
        )
    
    def validate_all(self) -> ValidationResult:
        """
        Perform complete validation of system and input data
        
        Returns:
            ValidationResult with all validation details
        """
        # Check system requirements
        system_issues = self.validate_system()
        
        # Validate each clip
        clip_results = {}
        total_size = 0
        
        for clip in self.manifest_parser.get_clips():
            result = self.validate_clip(clip)
            clip_results[clip.name] = result
            total_size += result.estimated_size
            
            # Update clip status based on validation
            if not result.segments:
                clip.status = ClipStatus.MISSING
            elif result.missing_segments:
                clip.status = ClipStatus.PARTIAL
            else:
                clip.status = ClipStatus.COMPLETE
        
        # Check available space
        try:
            available = shutil.disk_usage(self.output_dir).free
        except Exception:
            available = 0
            system_issues.append(ValidationIssue(
                level=ValidationLevel.ERROR,
                message="Cannot determine available disk space",
                help_text="Check output directory permissions"
            ))
        
        return ValidationResult(
            system_issues=system_issues,
            clip_results=clip_results,
            total_size_estimate=total_size,
            available_space=available
        )

----
Full Path: src/cosmos/utils.py

# File: src/cosmos/utils.py

import logging
import json
import subprocess
import shutil
import sys
from pathlib import Path
from typing import Any, Dict, Optional

__version__ = "0.2.1"  # Just an example version

def init_logging(level: str = "INFO", logfile: Optional[Path] = None) -> logging.Logger:
    logger = logging.getLogger("cosmos")
    logger.setLevel(getattr(logging, level.upper(), logging.INFO))
    logger.handlers.clear()
    logger.propagate = False

    ch = logging.StreamHandler(sys.stdout)
    ch.setLevel(getattr(logging, level.upper(), logging.INFO))
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(message)s")
    ch.setFormatter(formatter)
    logger.addHandler(ch)

    if logfile:
        fh = logging.FileHandler(logfile, encoding='utf-8')
        fh.setLevel(getattr(logging, level.upper(), logging.INFO))
        fh.setFormatter(formatter)
        logger.addHandler(fh)

    return logger

def load_config(config_path: Path) -> Dict[str, Any]:
    if config_path.is_file():
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, OSError):
            return {}
    return {}

def save_config(config_path: Path, config: Dict[str, Any]) -> None:
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2)

def check_ffmpeg() -> bool:
    try:
        subprocess.run(["ffmpeg", "-version"], check=True, capture_output=True)
        return True
    except (subprocess.CalledProcessError, FileNotFoundError):
        return False

def is_git_available() -> bool:
    return shutil.which("git") is not None

def check_for_updates(repo_path: Path) -> bool:
    """
    Check if updates are available for a Git repository.
    Returns True if an update is available, False otherwise.
    If any error occurs, returns False.
    """
    try:
        # 'git fetch' to update remote refs
        subprocess.run(["git", "-C", str(repo_path), "fetch"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        remote_head = subprocess.check_output(["git", "-C", str(repo_path), "ls-remote", "origin", "HEAD"], text=True).split()[0]
        local_head = subprocess.check_output(["git", "-C", str(repo_path), "rev-parse", "HEAD"], text=True).strip()
        return remote_head != local_head
    except Exception as e:
        print_error(f"Error checking for updates: {e}")
        return False

def check_updates(repo_path: Path = Path(".")) -> None:
    print_info(f"Current version: {__version__}")
    if is_git_available():
        if check_for_updates(repo_path):
            print_info("An update is available! Run `git pull` to update.")
        else:
            print_info("You are up-to-date.")
    else:
        print_warning("Git is not installed or not accessible. Cannot check for updates.")
        print_info("Visit the repository and `git pull` manually to update.")

def print_info(message: str) -> None:
    print(f"[INFO] {message}")

def print_warning(message: str) -> None:
    print(f"[WARNING] {message}")

def print_error(message: str) -> None:
    print(f"[ERROR] {message}", file=sys.stderr)

def print_success(message: str) -> None:
    print(f"[SUCCESS] {message}")


----
Full Path: src/cosmos/manifest.py

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import xml.etree.ElementTree as ET
import logging


class ClipStatus(Enum):
    """Status of a clip's data availability"""
    COMPLETE = "complete"          # All expected segments present
    PARTIAL = "partial"           # Some segments missing
    MISSING = "missing"           # No segments found
    INVALID = "invalid"           # Metadata inconsistency detected


@dataclass
class Position:
    """
    Represents a position in the COSM directory structure.
    
    Format is typically "NH/MM/SS.sss" where:
    - N: Hour number
    - MM: Minute number
    - SS.sss: Second with fractional component
    """
    hour: int
    minute: int
    second: float

    @classmethod
    def from_string(cls, pos_str: str) -> "Position":
        """Parse a position string like '0H/0M/3.8S/'"""
        parts = pos_str.strip('/').split('/')
        if len(parts) != 3:
            raise ValueError(f"Invalid position string format: {pos_str}")
        
        try:
            hour = int(parts[0].rstrip('H'))
            minute = int(parts[1].rstrip('M'))
            second = float(parts[2].rstrip('S'))
            return cls(hour=hour, minute=minute, second=second)
        except (ValueError, IndexError) as e:
            raise ValueError(f"Failed to parse position string: {pos_str}") from e

    def to_string(self) -> str:
        """Convert position back to string format"""
        return f"{self.hour}H/{self.minute}M/{self.second}S"
    
    def to_seconds(self) -> float:
        """Convert position to total seconds from start of hour"""
        return self.hour * 3600 + self.minute * 60 + self.second
    
    def path_fragment(self) -> str:
        """Return a directory path fragment like '0H/0M/25S'."""
        return f"{self.hour}H/{self.minute}M/{self.second}S"

@dataclass
class ClipInfo:
    """
    Information about a single clip from the manifest.
    
    Clips represent continuous recording sessions, each containing multiple
    video segments that should be processed together.
    """
    name: str                     # Clip identifier (e.g., "CLIP1")
    start_epoch: float           # Start timestamp (Unix epoch)
    end_epoch: float             # End timestamp (Unix epoch)
    start_pos: Position          # Starting directory position
    end_pos: Position            # Ending directory position
    start_idx: int              # Starting frame index
    end_idx: int                # Ending frame index
    start_time: datetime        # Human-readable start time
    status: ClipStatus = ClipStatus.MISSING

    @property
    def duration(self) -> float:
        """Duration of clip in seconds"""
        return self.end_epoch - self.start_epoch

    @property
    def frame_count(self) -> int:
        """Total number of frames in clip"""
        return self.end_idx - self.start_idx


class ManifestParser:
    """
    Parser for COSM C360 camera clip manifests.
    
    The manifest XML contains information about recording sessions ("clips"),
    including their temporal boundaries, frame indices, and positions within
    the directory structure.
    """
    
    def __init__(self, manifest_path: Path, logger: Optional[logging.Logger] = None):
        """
        Initialize parser with path to manifest XML.
        
        Args:
            manifest_path: Path to the manifest XML file
            logger: Optional logger instance
            
        Raises:
            FileNotFoundError: If manifest file doesn't exist
            xml.etree.ElementTree.ParseError: If XML is malformed
        """
        self.manifest_path = manifest_path
        self._clips: Dict[str, ClipInfo] = {}
        self.logger = logger or logging.getLogger(__name__)
        
        if not manifest_path.exists():
            raise FileNotFoundError(f"Manifest not found: {manifest_path}")
        
        self._parse_manifest()
    
    def _parse_manifest(self) -> None:
        """
        Parse the manifest XML file and extract clip information.
        
        The manifest contains a series of clip entries with attributes:
        - Name: Clip identifier
        - Epoch: Start time in Unix epoch format
        - Pos: Directory position string
        - InIdx/OutIdx: Frame index boundaries
        - InStr: Human-readable timestamp
        """
        tree = ET.parse(self.manifest_path)
        root = tree.getroot()
        
        self.logger.debug(f"Parsing manifest: {self.manifest_path}")
        self.logger.debug(f"Found {len(root)} clip entries in manifest")
        
        for elem in root:
            try:
                name = elem.attrib['Name']
                self.logger.debug(f"Parsing clip: {name}")
                self.logger.debug(f"Raw attributes: {elem.attrib}")
                
                start_epoch = float(elem.attrib['Epoch'])
                pos = Position.from_string(elem.attrib['Pos'])
                start_idx = int(elem.attrib['InIdx'])
                end_idx = int(elem.attrib['OutIdx'])
                start_time = datetime.strptime(
                    elem.attrib['InStr'],
                    "%H:%M:%S.%f %m/%d/%Y"
                )
                
                # Store clip info
                self._clips[name] = ClipInfo(
                    name=name,
                    start_epoch=start_epoch,
                    end_epoch=None,  # Will be determined from segment data
                    start_pos=pos,
                    end_pos=None,    # Will be determined from segment data
                    start_idx=start_idx,
                    end_idx=end_idx,
                    start_time=start_time
                )
                
                self.logger.debug(
                    f"Parsed {name}: start_epoch={start_epoch}, "
                    f"pos={pos.to_string()}, "
                    f"frame_range={start_idx}-{end_idx}"
                )
                
            except (KeyError, ValueError) as e:
                # Log warning but continue parsing other clips
                self.logger.warning(f"Warning: failed to parse clip element: {e}")
                continue
        
        self.logger.info(f"Successfully parsed {len(self._clips)} clips from manifest")
        for name, clip in self._clips.items():
            self.logger.info(
                f"Clip {name}: "
                f"start={clip.start_time}, "
                f"frames={clip.frame_count}, "
                f"pos={clip.start_pos.to_string()}"
            )
    
    def get_clip(self, name: str) -> Optional[ClipInfo]:
        """Get information for a specific clip by name"""
        return self._clips.get(name)
    
    def get_clips(self) -> List[ClipInfo]:
        """Get list of all clips in temporal order"""
        return sorted(
            self._clips.values(),
            key=lambda x: x.start_epoch
        )
    
    def update_clip_status(self, name: str, status: ClipStatus) -> None:
        """Update the status of a clip after validation"""
        if clip := self._clips.get(name):
            clip.status = status
    
    def find_clip_for_timestamp(self, timestamp: float) -> Optional[ClipInfo]:
        """Find the clip containing a given timestamp"""
        for clip in self._clips.values():
            if clip.start_epoch <= timestamp and (
                clip.end_epoch is None or timestamp <= clip.end_epoch
            ):
                return clip
        return None


def find_manifest(base_dir: Path) -> Optional[Path]:
    """
    Find the COSM manifest XML file in a directory.
    
    Args:
        base_dir: Directory to search for manifest
        
    Returns:
        Path to manifest if exactly one is found, None otherwise
        
    Raises:
        ValueError: If multiple manifest files are found
    """
    manifests = list(base_dir.glob("**/*.xml"))
    
    if not manifests:
        return None
    
    if len(manifests) > 1:
        raise ValueError(
            f"Multiple manifest files found: {', '.join(str(p) for p in manifests)}"
        )
    
    return manifests[0]

----
Full Path: src/cosmos/cli.py

# File: src/cosmos/cli.py

import argparse
import sys
from pathlib import Path
from typing import Optional, Dict, Any

from .utils import (
    init_logging,
    load_config,
    save_config,
    check_updates,
    check_ffmpeg,
    print_info,
    print_warning,
    print_error
)

try:
    import questionary
except ImportError:
    questionary = None

def parse_args():
    parser = argparse.ArgumentParser(description="COSM C360 Tools CLI")
    parser.add_argument("--input-dir", type=str, help="Path to input directory containing raw segments")
    parser.add_argument("--output-dir", type=str, help="Path to output directory where processed files will be saved")
    parser.add_argument("--manifest", type=str, help="Path to manifest file if not discoverable automatically")
    parser.add_argument("--config-file", type=str, help="Path to a configuration file (JSON)")
    parser.add_argument("--log-file", type=str, help="Path to a logfile")
    parser.add_argument("--log-level", type=str, default="INFO", help="Logging level (DEBUG, INFO, WARNING, ERROR)")
    parser.add_argument("--interactive", action="store_true", help="Run in interactive mode")
    parser.add_argument("--self-test", action="store_true", help="Run a self-test and exit")
    parser.add_argument("--check-updates", action="store_true", help="Check for updates and exit")
    parser.add_argument("--job-name", type=str, help="A name for this job, used for logs and output notes")

    return parser.parse_args()

def run_interactive_mode(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Run an interactive session to gather input_dir, output_dir, and other settings.
    Uses questionary for cross-platform interactive prompts.
    If questionary is not installed, print an error and exit.
    """
    if questionary is None:
        print_error("Interactive mode requested but 'questionary' is not installed.")
        sys.exit(1)

    print_info("Entering interactive mode...")

    # Prompt for input directory
    input_dir = questionary.path(
        "Please select the input directory containing raw segments:",
        default=str(config.get("input_dir", ""))
    ).ask()

    if not input_dir:
        print_error("No input directory specified.")
        sys.exit(1)

    # Prompt for output directory
    output_dir = questionary.path(
        "Please select the output directory for processed files:",
        default=str(config.get("output_dir", ""))
    ).ask()

    if not output_dir:
        print_error("No output directory specified.")
        sys.exit(1)

    # Confirm ffmpeg available
    if not check_ffmpeg():
        print_warning("FFmpeg not found. Please install it before proceeding.")
        cont = questionary.confirm("Do you want to continue anyway?", default=False).ask()
        if not cont:
            print_info("Aborted due to missing FFmpeg.")
            sys.exit(1)

    # Prompt for log level
    log_level = questionary.select(
        "Select log level:",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default=config.get("log_level", "INFO")
    ).ask()

    # Prompt for job name
    job_name = questionary.text(
        "Job name (optional):",
        default=str(config.get("job_name", ""))
    ).ask()

    config["input_dir"] = input_dir
    config["output_dir"] = output_dir
    config["log_level"] = log_level
    if job_name:
        config["job_name"] = job_name

    print_info("Review your settings:")
    print_info(f"  Input Directory: {config['input_dir']}")
    print_info(f"  Output Directory: {config['output_dir']}")
    print_info(f"  Log Level: {config['log_level']}")
    if "job_name" in config:
        print_info(f"  Job Name: {config['job_name']}")
    confirm = questionary.confirm("Proceed with these settings?", default=True).ask()

    if not confirm:
        print_info("Aborted by user.")
        sys.exit(0)

    return config

def run_cli() -> Dict[str, Any]:
    """
    Parse command-line arguments, handle interactive mode if requested,
    load configuration files if provided, and return a final config dict
    for use by cosmos.py.
    """
    args = parse_args()

    # Load config if provided
    config_path = Path(args.config_file) if args.config_file else None
    config = load_config(config_path) if config_path else {}

    # CLI args override config values
    if args.log_level:
        config["log_level"] = args.log_level
    if args.log_file:
        config["log_file"] = args.log_file
    if args.input_dir:
        config["input_dir"] = args.input_dir
    if args.output_dir:
        config["output_dir"] = args.output_dir
    if args.manifest:
        config["manifest"] = args.manifest
    if args.job_name:
        config["job_name"] = args.job_name

    # Initialize logging early so we can log further steps
    logfile = Path(config["log_file"]) if "log_file" in config else None
    logger = init_logging(level=config.get("log_level", "INFO"), logfile=logfile)

    # Handle special commands that do not require proceeding to processing
    # (NOTE: Self-test is now handled in cosmos.py, not here)
    if args.check_updates:
        check_updates()
        sys.exit(0)

    # If interactive mode requested, run interactive prompts
    if args.interactive:
        config = run_interactive_mode(config)

    # Validate directories are set
    if "input_dir" not in config or not config["input_dir"]:
        print_error("Input directory not specified. Use --input-dir or run in interactive mode.")
        sys.exit(1)

    if "output_dir" not in config or not config["output_dir"]:
        print_error("Output directory not specified. Use --output-dir or run in interactive mode.")
        sys.exit(1)

    # Save updated config if config file provided
    if config_path:
        save_config(config_path, config)

    logger.info("CLI argument resolution complete. Ready to proceed with processing pipeline.")
    logger.info(f"Input Directory: {config['input_dir']}")
    logger.info(f"Output Directory: {config['output_dir']}")
    if "manifest" in config:
        logger.info(f"Manifest: {config['manifest']}")

    # Add self_test key so cosmos.py can check and handle it
    config["self_test"] = args.self_test

    return config


----
Full Path: src/cosmos/preflight.py

# File: src/cosmos/preflight.py

import platform
import os
import sys
import shutil
from pathlib import Path
import psutil
from typing import Optional

from .utils import print_info, print_warning, print_error, print_success, check_ffmpeg

def check_python_version(min_version=(3, 8)):
    return sys.version_info >= min_version

def check_system_memory(minimum_gb=8):
    total_memory = psutil.virtual_memory().total / (1024**3)
    return total_memory >= minimum_gb

def check_disk_space(directory: Path, minimum_gb=10):
    usage = shutil.disk_usage(directory)
    free_gb = usage.free / (1024**3)
    return free_gb >= minimum_gb

def check_required_codecs():
    # Assume ffmpeg install that we verified can run. Detailed codec check
    # could parse `ffmpeg -encoders` but omitted for brevity.
    return check_ffmpeg()

def check_directory_structure(input_dir: Path, manifest_path: Optional[Path] = None):
    """
    Basic directory structure checks:
    - Confirm input_dir exists
    - Confirm we have at least one .xml manifest if manifest_path not given
    - If multiple .xml manifests found and no manifest_path specified, fail
    - Check if we have at least one directory structure like NH/*M/*S/meta.json
    """
    if not input_dir.is_dir():
        return {
            "Directory Exists": {
                "check": False,
                "message": f"Input directory {input_dir} does not exist.",
                "help": "Check the input_dir path."
            }
        }
    
    # Check for manifest files if manifest_path not specified
    manifests = list(input_dir.glob("*.xml"))
    if manifest_path:
        # User specified manifest; check if it exists
        if not manifest_path.is_file():
            return {
                "Manifest Provided": {
                    "check": False,
                    "message": f"Specified manifest {manifest_path} not found.",
                    "help": "Ensure you provided the correct manifest path."
                }
            }
    else:
        # No manifest specified; must find exactly one
        if len(manifests) == 0:
            return {
                "Manifest Discovery": {
                    "check": False,
                    "message": "No .xml manifest found in top-level directory.",
                    "help": "Provide --manifest or place a single .xml in input_dir."
                }
            }
        elif len(manifests) > 1:
            return {
                "Manifest Ambiguity": {
                    "check": False,
                    "message": "Multiple .xml manifests found but none specified.",
                    "help": "Use --manifest to specify which manifest to use."
                }
            }

    # Check basic structure: at least one H directory, inside it at least one M directory, inside it at least one S directory with meta.json
    hour_dirs = [d for d in input_dir.glob("*H") if d.is_dir()]
    if not hour_dirs:
        return {
            "Directory Structure": {
                "check": False,
                "message": "No hour-level (e.g. '0H') directories found.",
                "help": "Ensure input_dir follows the 'NH/MM/SS' structure."
            }
        }

    # Check at least one M directory
    found_valid_structure = False
    for hdir in hour_dirs:
        mdirs = [m for m in hdir.glob("*M") if m.is_dir()]
        for mdir in mdirs:
            sdirs = [s for s in mdir.glob("*S") if s.is_dir()]
            for sdir in sdirs:
                meta = sdir / "meta.json"
                if meta.is_file():
                    found_valid_structure = True
                    break
            if found_valid_structure:
                break
        if found_valid_structure:
            break

    if not found_valid_structure:
        return {
            "Directory Structure": {
                "check": False,
                "message": "Did not find any second-level directories with meta.json.",
                "help": "Ensure the directory structure matches expected format (0H/0M/0S/meta.json)."
            }
        }

    return {
        "Directory Structure": {
            "check": True
        }
    }

def check_windows_specific(input_dir: Path):
    """
    On Windows, check if any paths exceed 260 chars.
    This can cause issues on older Windows environments.
    """
    if platform.system() == "Windows":
        for root, dirs, files in os.walk(input_dir):
            for name in dirs + files:
                full_path = Path(root, name)
                if len(str(full_path)) > 260:
                    return {
                        "Path Length": {
                            "check": False,
                            "message": f"Path exceeds 260 characters: {full_path}",
                            "help": "Shorten directory names or enable long paths in Windows."
                        }
                    }
    return {
        "Path Length": {"check": True}
    }

def validate_system_requirements(input_dir: Path, output_dir: Path):
    checks = {
        "FFmpeg Installation": {
            "check": check_ffmpeg(),
            "message": "FFmpeg not found. Please install it.",
            "help": "https://ffmpeg.org/download.html"
        },
        "HEVC Codec Availability": {
            "check": check_required_codecs(),
            "message": "HEVC codec support not found in ffmpeg.",
            "help": "Ensure your ffmpeg build supports HEVC."
        },
        "Python Version": {
            "check": check_python_version(),
            "message": "Python 3.8 or higher required.",
            "help": "Please upgrade your Python installation."
        },
        "System Memory": {
            "check": check_system_memory(minimum_gb=8),
            "message": "Less than 8GB RAM available. Processing may be slow.",
            "help": "Use --low-memory mode or upgrade system memory."
        },
        "Disk Space": {
            "check": check_disk_space(output_dir, minimum_gb=10),
            "message": "Less than 10GB disk space available.",
            "help": "Free disk space or choose another output directory."
        }
    }
    return checks

def format_validation_results(validation_results):
    all_good = True
    for category, result in validation_results.items():
        if not result["check"]:
            all_good = False
            print_error(f"[{category}] {result['message']} - {result.get('help', '')}")
        else:
            print_success(f"[{category}] OK")
    return all_good

def run_self_test(input_dir: Path, output_dir: Path, manifest_path: Optional[Path] = None) -> bool:
    print_info("Running system pre-flight checks...")
    system_checks = validate_system_requirements(input_dir, output_dir)
    sys_ok = format_validation_results(system_checks)

    print_info("Checking basic directory structure...")
    dir_checks = check_directory_structure(input_dir, manifest_path)
    dir_ok = format_validation_results(dir_checks)

    windows_ok = True
    if platform.system() == "Windows":
        print_info("Performing Windows-specific checks...")
        win_checks = check_windows_specific(input_dir)
        windows_ok = format_validation_results(win_checks)

    return sys_ok and dir_ok and windows_ok


----
Full Path: src/cosmos/processor.py

import os
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
import subprocess
import tempfile
from datetime import datetime
import logging

from .validation import ClipValidationResult, SegmentInfo
from .manifest import ClipInfo

class ProcessingMode(Enum):
    QUALITY = "quality"        # Highest quality, all threads
    BALANCED = "balanced"      # Good quality, all threads
    PERFORMANCE = "speed"      # Faster, all threads
    LOW_MEMORY = "low_memory"  # Half threads
    MINIMAL = "minimal"        # Single thread

@dataclass
class ProcessingOptions:
    """Configuration for video processing"""
    output_resolution: Tuple[int, int]  # Width, height
    quality_mode: ProcessingMode
    low_memory: bool = False
    crf: Optional[int] = None  # Custom CRF value if specified

class EncoderType(Enum):
    """Available encoder types"""
    NVIDIA_NVENC = "h264_nvenc"
    AMD_AMF = "h264_amf"
    INTEL_QSV = "h264_qsv"
    SOFTWARE_X264 = "libx264"

@dataclass
class ProcessingResult:
    """Results from processing a clip"""
    clip: ClipInfo
    output_path: Path
    duration: float
    frames_processed: int
    success: bool
    error: Optional[str] = None

class VideoProcessor:
    """
    Handles video processing operations for COSM camera output.
    
    This class manages:
    - Segment concatenation
    - Tile extraction and alignment
    - Frame assembly
    - Output encoding
    """
    
    def __init__(self, 
                 output_dir: Path,
                 options: ProcessingOptions,
                 logger: Optional[logging.Logger] = None):
        self.output_dir = output_dir
        self.options = options
        self.logger = logger or logging.getLogger(__name__)
        self._available_encoders = self._detect_encoders()
        
        self.logger.debug(f"Initialized VideoProcessor with options: {options}")
        self.logger.debug(f"Available encoders: {[e.value for e in self._available_encoders]}")

    def _detect_encoders(self) -> List[EncoderType]:
        """
        Detect available encoders on the system.
        Returns list of encoders in preferred order.
        """
        available = []
        try:
            # Query ffmpeg for encoder list
            result = subprocess.run(
                ["ffmpeg", "-encoders"],
                capture_output=True,
                text=True,
                check=True
            )
            
            # Parse output to detect available encoders
            output = result.stdout.lower()
            
            # Check for hardware encoders first
            if "h264_nvenc" in output:
                available.append(EncoderType.NVIDIA_NVENC)
            if "h264_amf" in output:
                available.append(EncoderType.AMD_AMF)
            if "h264_qsv" in output:
                available.append(EncoderType.INTEL_QSV)
            
            # Software encoder should always be available
            available.append(EncoderType.SOFTWARE_X264)
            
        except subprocess.SubprocessError:
            # If ffmpeg query fails, default to software encoding
            self.logger.warning("Failed to detect encoders, defaulting to software")
            available.append(EncoderType.SOFTWARE_X264)
            
        return available

    def _get_encoder_settings(self, 
                            encoder: EncoderType,
                            thread_count: Optional[int] = None) -> List[str]:
        """
        Get ffmpeg arguments for specified encoder
        
        Args:
            encoder: Encoder to use
            thread_count: Number of threads to use (None for auto)
        """
        # Base quality settings
        crf = self.options.crf or {
            ProcessingMode.QUALITY: 18,
            ProcessingMode.BALANCED: 23,
            ProcessingMode.PERFORMANCE: 28
        }[self.options.quality_mode]
        
        # Start with encoder-specific settings
        if encoder == EncoderType.NVIDIA_NVENC:
            settings = [
                "-c:v", "h264_nvenc",
                "-preset", "p7" if self.options.quality_mode == ProcessingMode.QUALITY else "p4",
                "-qp", str(crf)
            ]
        elif encoder == EncoderType.SOFTWARE_X264:
            settings = [
                "-c:v", "libx264",
                "-preset", "slower" if self.options.quality_mode == ProcessingMode.QUALITY else "medium",
                "-crf", str(crf)
            ]
            
            # Add thread control for software encoding
            if thread_count is not None:
                settings.extend([
                    "-threads", str(thread_count),
                    "-x264-params", f"threads={thread_count}"
                ])
        else:
            # Default settings for other encoders
            settings = ["-c:v", "libx264", "-crf", str(crf)]
        
        return settings

    def _build_filter_complex(self, 
                            crop_overlap: int = 32) -> str:
        """Build ffmpeg filter complex for tile processing."""
        filter_complex = (
            # Crop overlapping regions from tiles
            "[0:v:0]crop=iw-{overlap}:ih-{overlap}:0:0[tl];"
            "[0:v:1]crop=iw-{overlap}:ih-{overlap}:{overlap}:0[tr];"
            "[0:v:2]crop=iw-{overlap}:ih-{overlap}:0:{overlap}[bl];"
            "[0:v:3]crop=iw-{overlap}:ih-{overlap}:{overlap}:{overlap}[br];"
            # Stack tiles horizontally
            "[tl][tr]hstack=2[top];"
            "[bl][br]hstack=2[bottom];"
            # Stack rows vertically
            "[top][bottom]vstack=2"
        ).format(overlap=crop_overlap)
        
        self.logger.debug(f"Generated filter complex:\n{filter_complex}")
        return filter_complex

    def _create_concat_file(self, segments: List[SegmentInfo]) -> Path:
        """Create temporary concat file for ffmpeg"""
        # Use platform-agnostic temp file creation
        temp_file = Path(tempfile.mktemp(suffix='.txt'))
        
        with open(temp_file, 'w', encoding='utf-8') as f:
            for segment in segments:
                for ts_file in segment.ts_files:
                    # Use forward slashes even on Windows
                    path_str = str(ts_file.absolute()).replace('\\', '/')
                    f.write(f"file '{path_str}'\n")
                    
        return temp_file

    def process_clip(self,
                    clip_result: ClipValidationResult) -> ProcessingResult:
        """Process a validated clip."""
        try:
            output_path = self.output_dir / f"{clip_result.clip.name}.mp4"
            concat_file = self._create_concat_file(clip_result.segments)
            
            # Determine thread count for software encoding
            if self.options.low_memory:
                import multiprocessing
                total_threads = multiprocessing.cpu_count()
                # Use half threads in low memory mode
                thread_count = max(1, total_threads // 2)
            else:
                thread_count = None
                
            self.logger.debug(f"Processing clip {clip_result.clip.name}")
            self.logger.debug(f"Created concat file at {concat_file}")
            self.logger.debug(f"Output will be written to {output_path}")
            
            # Log concat file contents for debugging
            with open(concat_file, 'r') as f:
                self.logger.debug(f"Concat file contents:\n{f.read()}")
                
            success = False
            error_messages = []
            
            for encoder in self._available_encoders:
                try:
                    # Build base command
                    cmd = [
                        "ffmpeg", "-y",
                        "-f", "concat",
                        "-safe", "0",
                        "-i", str(concat_file),
                        "-filter_complex", self._build_filter_complex()
                    ]
                    
                    # Add encoder settings with thread control for software encoding
                    use_threads = thread_count if (
                        encoder == EncoderType.SOFTWARE_X264 and 
                        self.options.low_memory
                    ) else None
                    
                    cmd.extend(self._get_encoder_settings(encoder, use_threads))
                    cmd.append(str(output_path))
                    
                    # Run ffmpeg with proper subprocess configuration for Windows
                    self.logger.info(f"Processing {clip_result.clip.name} with {encoder.value}")
                    # Log the complete ffmpeg command
                    self.logger.debug(f"Executing ffmpeg command:\n{' '.join(cmd)}")
                    subprocess.run(
                        cmd,
                        check=True,
                        capture_output=True,
                        text=True,
                        encoding='utf-8',
                        errors='replace',
                        creationflags=subprocess.CREATE_NO_WINDOW if os.name == 'nt' else 0
                    )
                    
                    # # Log ffmpeg output
                    # if result.stdout:
                    #     self.logger.debug(f"FFmpeg stdout:\n{result.stdout}")
                    # if result.stderr:
                    #     self.logger.debug(f"FFmpeg stderr:\n{result.stderr}")
                    
                    success = True
                    break
                    
                except subprocess.SubprocessError as e:
                    error_msg = f"{encoder.value}: {e}"
                    self.logger.error(f"Encoder failed: {error_msg}")
                    error_messages.append(error_msg)
                    continue
                
            if not success:
                raise RuntimeError(
                    f"All encoders failed: {'; '.join(error_messages)}"
                )
            
            # Calculate processing statistics
            duration = clip_result.clip.duration
            frames = sum(seg.frame_count for seg in clip_result.segments)
            
            return ProcessingResult(
                clip=clip_result.clip,
                output_path=output_path,
                duration=duration,
                frames_processed=frames,
                success=True
            )
            
        except Exception as e:
            self.logger.error(f"Error processing {clip_result.clip.name}: {e}")
            return ProcessingResult(
                clip=clip_result.clip,
                output_path=None,
                duration=0,
                frames_processed=0,
                success=False,
                error=str(e)
            )
            
        finally:
            # Cleanup
            if 'concat_file' in locals():
                concat_file.unlink()

----
Full Path: src/cosmos/__init__.py



----
Full Path: tests/test_data/ladybird/LADYBIRD.xml

<Clip_Manifest NumDirs="498">
  <_1 Name="CLIP2" In="1.6261465427355111E-307" InIdx="228" Out="1.6261466707242047E-307" OutIdx="4110" Locked="True" InStr="07:27:16.618 08/13/2024" Epoch="1723559236.618" Pos="0H/0M/3.8S/" />
  <_2 Name="CLIP1" In="1.6261465850368715E-307" InIdx="1511" Out="1.6261470057932999E-307" OutIdx="14273" Locked="True" InStr="07:27:38.022 08/13/2024" Epoch="1723559258.0219998" Pos="0H/0M/25.1833333333333S/" />
  <_3 Name="CLIP3" In="1.6261501214407288E-307" InIdx="14658" Out="1.6261501214407288E-307" OutIdx="14658" Locked="True" InStr="07:57:27.463 08/13/2024" Epoch="1723561047.4629998" Pos="0H/4M/4.30000000000001S/" />
  <_4 Name="CLIP4" In="1.6261504029336424E-307" InIdx="15096" Out="1.6261504567393675E-307" OutIdx="16728" Locked="True" InStr="07:59:49.900 08/13/2024" Epoch="1723561189.8999999" Pos="0H/4M/11.6S/" />
  <_5 Name="CLIP6" In="1.626151890120643E-307" InIdx="17124" Out="1.6261520105896575E-307" OutIdx="20778" Locked="True" InStr="08:12:22.425 08/13/2024" Epoch="1723561942.425" Pos="0H/4M/45.4S/" />
  <_6 Name="CLIP7" In="1.6261529151745092E-307" InIdx="21221" Out="1.6261531935026773E-307" OutIdx="29663" Locked="True" InStr="08:21:01.108 08/13/2024" Epoch="1723562461.108" Pos="0H/5M/53.6833333333333S/" />
</Clip_Manifest>

----
Full Path: tests/test_data/ladybird/0H/0M/0S/meta.json

{
"Time":{"x0":1723559232.815, "xi-x0":[0,0.016,0.033,0.050,0.066,0.083,0.100,0.116,0.133,0.150,0.166,0.183,0.200,0.216,0.233,0.250,0.267,0.283,0.300,0.317,0.333,0.350,0.367,0.383,0.400,0.417,0.433,0.450,0.467,0.483,0.500,0.517,0.533,0.550,0.567,0.584,0.600,0.617,0.634,0.650,0.667,0.684,0.700,0.717,0.734,0.750,0.767,0.784,0.800,0.817,0.834,0.850,0.867,0.884,0.900,0.917,0.934,0.951,0.967,0.984]}
}

